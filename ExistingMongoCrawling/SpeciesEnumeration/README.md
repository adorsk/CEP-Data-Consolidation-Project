# Species Enumeration README

Alex gave me the idea to create this file before I leave the CEP project. The purpose of this README is to document the code from the last stage of my project -- to enumerate all the different species of data that can be found in all the existing CEP mongo databases. This file will be very useful and time-saving for whoever continues the project after I have left. 

## List of Files with Descriptions

- [remote_connector_template.py](/ExistingMongoCrawling/SpeciesEnumeration/remote_connector_template.py) This is a template Python script to connect to the molspace.rc.fas.harvard.edu server containing the existing CEP mongo databases. It handles connecting to the database (taking in the username and password as command line arguments), as well as closing the connection cleanly upon exit (even when that exit is done with a `Ctrl-C` command). It is highly recommended that if you want to use Pymongo to connect to the remote databases, to base your code off this script. This basic framework is tried and tested. 

- [species_enumerator.py](/ExistingMongoCrawling/SpeciesEnumeration/species_enumerator.py) This is the Python script used to poll each collection of each database of a mongo server and determine all the species (i.e. structures of documents) in each collection. Note that it works off the localhost, since I already copied all the CEP mongo data to my local machine using the scripts [here](/MigrateDataToLocalMachine). Note that for the sake of efficiency, this script does not read through every single document in the collection, instead using a probabilistic approach. The `process_collection()` method is somewhat slow for small databases (i.e. 1 million records) but scales linearly, allowing all 1.5 Tb of data to be gone through in about 12 hours. The `new_process_collection()` method is more thorough in the proportion of documents per collection it polls, and thus has a lower chance of missing rare species. It runs at about the same speed as `process_collection()` for databases of about 1 million records, but it scales quasilinearly (i.e. `n log n`) and is therefore much slower for the larger databases. This script dumps its output in the [`output`](/ExistingMongoCrawling/SpeciesEnumeration/output) folder. 
